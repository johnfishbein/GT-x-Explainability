\section{Conclusion}

While the fields of cooperative game theory and model explainability may seem entirely disconnected to the layman, their connection has been made evident by several waves of excellent researchers. The elegant power of the Shapley value in solving many of the longstanding problems in model explainability was first published by \citet{regressionInGameTheory_MS4}. Their work made the connection known, but failed to attract significant attention at the time, and was lacking in both generality and in the sophistication of their Shapley value approximation techniques. \citet{strumbelj_efficient_2010} were the first group to broaden this approach to apply to arbitrary classification models, though they did little to innovate on previous Shapley estimation methodology. \citet{QII_MS3}'s work went even further in the generalization of the cooperative game theoretic approach to model explainability, and was the first very widely noticed paper in the field.
Finally, \citet{microsoftPaper} tied all the previous work together, creating a fully general definition of explanation models and unifying all notable past approaches under a class of models they call additive feature attribution models. They went on to show how a technique previously used to attribute importance to model inputs \citep{ribeiroSG16_MS5}, which seemingly had no connection to game theory, could be carefully tuned to efficiently and accurately recover the Shapley values of a model's inputs. Since then, other papers have attempted to push the boundaries of this field even further, but these seminal papers brought this area of study into the mainstream, and spawned off startups \citep{QII_MS3} and open source projects \citep{microsoftPaper} which aim to use these profound results to effect meaningful change beyond academia.