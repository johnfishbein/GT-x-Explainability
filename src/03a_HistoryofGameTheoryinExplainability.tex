\subsection{History of Cooperative Game Theory in Model Explainability}
\label{sec:history}

As best we were able to tell in our research, the earliest instance of using game theory in the context of model explainability was from \citet{regressionInGameTheory_MS4}. In their paper, entitled \textit{Analysis of Regression in Game Theory Approach}, the authors propose the Shapley value as a method for evaluating feature importance specifically in the context of multiple regression. They say that while there exist numerous well known methods of feature importance evaluation tools, like the \textit{t}-tests and the coefficient of multiple determination, there had not previously existed any methods which were able to properly handle multicollinearity. This leads to significant undesirable and unpredictable results in variable importance estimates. They propose the Shapley value \eqref{shapley_value} as a novel method of quantifying the importance of coefficient in multiple regression which is uniquely effective in the presence of multicollinearity. A significant hurdle in applying the Shapley value to explain more general models is that calculating the Shapley value precisely is a combinatorially large \#P-complete problem \citep{shapley_complexity}, which is infeasible for models with thousands of input parameters. Under the general framework proposed by a more recent paper in this area \citep{microsoftPaper} (detailed in \hyperref[sec:lundberg]{Section 3.3}), the exact Shapley value for a particular covariate $i$ is related to the change in model prediction when the covariate is added to the predictive model's input. Particularly, because the order in which the covariates are sequentially added back to the model is important, it is the average of these output changes over all possible orderings of adding the covariates to the model input. \citet{regressionInGameTheory_MS4} propose sampling from the set of necessary computations in such a way that the Shapley value can be approximated. The paper by \citet{microsoftPaper} actually showed this to be a fairly effective approach in more complex settings, though somewhat inefficient and not as accurate as the newer paper's novel proposal.

Furthermore, the first instance we were able to uncover which used the Shapley value to put forth a more general framework of model explainability was the work of \citet{strumbelj_efficient_2010} in 2010. Alternatively to prior attempts at model explainability, they proposed a general, model agnostic approach to explain individual predictions. Their solution frames the problem of explaining predictions as a cooperative game, and uses the Shapley value to reveal the influence of given feature values. While this work made groundbreaking strides bridging the gap between Explainability and Cooperative game theory, the proposed solution still lacks a level of generality to be desired.  First, the solution provides general explainability to the predictions of a classifier, but there are cases in which this is not sufficient. As seen in the many cases of algorithmic bias in today's landscape, it is necessary to generalize the quantity being explained to account for more abstract settings other than simply the prediction of the model.  Furthermore, this proposed framework operates under the assumption that feature values are drawn from uniform distribution over the feature values. This assumption in some cases can cause the overall explanation to not adequately reflect the correlation between features that can exist in models in practice. They used randomized approximation algorithms based on similar types of sampling techniques first published by \citet{regressionInGameTheory_MS4}, and demonstrate that these are effective to within certain bounds. As we will discuss below, these levels of abstraction have since been researched further and improved upon.








