\section{Motivation for Model Explainability}

Model explainability is an incredibly important field. As models grow increasingly complex, they also grow increasingly opaque, and the reasons behind their actions can be difficult to decipher. This can lead to biases against races, genders, and other social groups, which can be hard to spot and harder to remedy. For simpler models this is not a significant problem, because the models themselves can be readily interpreted by a human, and no additional model descriptors are necessary to understand its motivations. For instance, consider a multiple linear regression model. The learned regression coefficients clearly relate the importance of a covariate as a predictor of the response variable via their magnitude, and convey the positivity or negativity of the covariate's correlation with the response variable via their sign. However, no such property of simple self description exists for more complicated models like random forests, other ensembling techniques, or ANNs. These intricate model classes require a higher level descriptor that captures the same easy-to-understand interpretation that is self contained in a traditional linear model \citep{microsoftPaper}. 


A example of this algorithmic bias has been observed in advertisement recommendation software. A widely accepted use case of machine learning techniques is advertisement recommendation software. Companies like Google and Facebook use their user’s data to produce predictions of what types of Ads a given user will respond positively to.  Google will then sell these predictions to advertisers and other interested parties for the purpose of optimizing some metric related to Ad revenue.  In 2015, \citet{adfisher} clearly demonstrated that these mechanisms operate with algorithmic bias.  Through several carefully executed experiments, they modified several aspects of google user profiles and observed the ads that were seen by these fake users.  In their first study, they randomly created 1000 users and set 500 of them to have a “male” gender and the other “500” to have a female gender. Then, after recording the resulting ads that were seen, they found that the most commonly seen ad for the “male” group was an ad for an executive level career coaching service. They reported that 402 out of the 500 male users were shown this ad at least once, but only 60 out of the 500 female users received the same ad. Clearly, the advertisement recommendation service considers the gender feature as relevant in the advertisement recommendation, and in the case of this particular ad, the predictions contain algorithmic bias \citep{adfisher}. 